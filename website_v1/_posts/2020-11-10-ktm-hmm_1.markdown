---
layout: post
title: "Know The Math - The Hidden Markov Model & the Forward Algorithm"
date: 2020-11-10 22:00:00 +0530
tags: tech machine_learning know_the_math
description: ' "Tareekh pe tareekh, tareekh pe tareekh, tareekh pe tareekh milti gayi My Lord, par insaaf nahi mila" - Sunny Deol from movie Damini '
---

[hmm_components]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/hmm_components.png
[hmm_first_order_assumptions]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/hmm_assumptions.jpg
[understanding_the_case]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/case_intro.PNG
[hidden_state_permutation]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/hidden_state_permutations.jpg
[forward_algorithm_1]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/forward_algorithm_1.PNG
[forward_algorithm_2]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/forward_algorithm_2.jpg
[forward_algorithm_3]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/forward_algorithm_3.PNG
[forward_algorithm_4]: ../assets/post_imgs/2020-11-10-ktm-hmm_1/forward_algorithm_4.jpg
[jurafskys_work]: https://web.stanford.edu/~jurafsky/slp3/A.pdf
[eisner_work]: https://cs.jhu.edu/~jason/papers/eisner.tnlp02.pdf

Hello reader, welcome to my _Know The Math_ series, a less formal series of blogs, where I take you through the *notoriously complex* backend mathematics which forms the soul for machine learning algorithms. I try and summarise works of famous researchers, put them as my notes and share with everyone in hopes that it might be helpful for you too!

Before we begin, I need you to conduct a ceremony:

1. Stand up in front of a mirror, and say it out aloud, _"I am going to learn the Math behind HMM & the Forward Algorithm, today!"_
2. Take a out a naice register & pen to go through the derivation with me.

## Contents

1. [Introduction](#introduction)
2. [The components of Hidden Markov Model](#the-components-of-hidden-markov-model)
3. [First-Order HMM & its Assumptions](#first-order-hmm--its-assumptions)
4. [3 Fundamental problems of HMM](#3-fundamental-problems-of-hmm)
5. [Understanding the Case & Example](#understanding-the-case--example)
6. [The Forward Algorithm](#the-forward-algorithm)
7. [References](#references)

## Introduction

A Hidden Markov Model helps us operate on probability distribution on a given sequence (more often a times-series) where the sequence is assumed to be a Markov process. This mini-blog aims to explore this, and it's more like my personal note on the concept which I am sharing with all. I've learnt almost entirity of the concept from the great [Professor Daniel Jurafsky](https://en.wikipedia.org/wiki/Dan_Jurafsky) and will be borrowing the content from his works available on the net. I'll try and summarise his work given over [here][jurafskys_work], as quick notes!

## The components of Hidden Markov Model

![HMM Components][hmm_components]
*HMM Components, source: [Jurafsky's work][jurafskys_work]*

## First-Order HMM & its Assumptions

1. **Markov Assumption**: The probability of particular state depends _only_ on previous state.
2. **Output Independence**: The probability of a particular observation **O_i** depends only on the state that produced the observation **q_i** & **not** on any other states or any other observations.

![First Order HMM & its assumptions][hmm_first_order_assumptions]
*First Order HMM & its assumptions, source: [Jurafsky's work][jurafskys_work]*

## 3 Fundamental problems of HMM

The HMM helps us deal with 3 fundamental problems. Each has its unique ways to go about it.

1. **Problem 1** - _Likelihood_ - Given an HMM λ = (A,B) and an observation sequence O, determine the likelihood P(O\|λ).
2. **Problem 2** - _Decoding_ - Given an observation sequence O and an HMM λ = (A,B), discover the best hidden state sequence Q.
3. **Problem 3** - _Learning_ - Given an observation sequence O and the set of states in the HMM, learn the HMM parameters A and B.

## Understanding the Case & Example

Jurafsky's work employs [Jason Eisner's work][eisner_work], and uses the following example to build and explain the above-mentioned problems.

Let's say that you are a climatologist from a distant future and are interested in weather in Baltimore, Maryland, for the summer of 2020. What you _don't_ have is the weather record from this time, but what you _do_ have is Eisner's diary from that time which details as to how many ice-creams Eisner had every day during that summer. So here, the number of ice-creams Eisner had everyday is the sequence of observations that we witness, and the temperature on every corresponding day which have no clue of (i.e. _hidden_ to us) is the sequence of hidden states which influence Eisner for the number of ice creams he eats. For simplicity, we'll have only 2 type of days - Hot or Cold, and Eisner can have maximum of 3 and minimum of 1 ice-cream on any given day. Naturally, one can understand that on Hot days Eisner is more probable to eat ice creams and opposite for Cold days.

+ _**Q = {H, C}**_ # The Hidden States
+ _**V = {1,2,3}**_ # The vocabulary or The alphabet from which an observation sequence can be generated

![Understanding the case][understanding_the_case]
*Understanding the given case, source: [Jurafsky's work][jurafskys_work]*

## The Forward Algorithm

This algorithm will help us solve the first problem as mentioned above. From the example, we can informally state the problem as let's say, _What is the probability of observing the sequence -: 3 - 1 - 3 ?_ What is the probability that Eisner eats 3 ice creams the 1st day, 1 ice cream the 2nd day and 3 ice creams the 3rd day? Now, what about the underlying hidden states? Each of those days, it can either be _Cold_ or _Hot_, and since there are 2 possible days in sequence, we have 8 (2^3) possibilites/permutation of the hidden states.

![Permutations of Hidden States][hidden_state_permutation]
*The possible hidden state combinations that can result in our observed sequence of 3-1-3*

> For HMM with N hidden states and an observation sequence of length T, there are N^T possible hidden sequences.

So to find out the likelihood of the given observation sequence, we have the Forward Algorithm which has a worst case time complexity of O(TN^2) optimised by Dynamic Programming. We are going to generate all possible sequences and take sum over their probabilities to arrive at our desired likelihood.

![Seeing how we explore all hidden sequences and find out all probabilties][forward_algorithm_1]
*Seeing how we explore all hidden sequences and find out all probabilties, source: [Jurafsky's work][jurafskys_work]*

In the algorithm, probability at every hidden state has been simplified as _alpha_ functions.

![The Alpha function][forward_algorithm_2]
*The Alpha function, source: [Jurafsky's work][jurafskys_work]*

The algorithm in whole, can be generalised as:

![Generalising the computations][forward_algorithm_3]
*Generalising the computations, source: [Jurafsky's work][jurafskys_work]*

![Algorithm Pseudocode & Time Series Analysis][forward_algorithm_4]
*Algorithm Pseudocode & Time Series Analysis*

Hope I was able to help you out with the concepts mentioned! I would also request to have a look at the original work, since this is just a summarised version of it for quick revision. Thank you so much Professor Jurafsky for the pathbreaking work.

I'll be continuing the HMM series, we'll talk about the Problem 2 next & the Viterbi Algorithm to solve it!

## References

+ <https://web.stanford.edu/~jurafsky/slp3/A.pdf>
+ <https://cs.jhu.edu/~jason/papers/eisner.tnlp02.pdf>
+ <http://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf>
+ <https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781>

> यस्त्वात्मरतिरेव स्यादात्मतृप्तश्च मानवः ।   
> आत्मन्येव च सन्तुष्टस्तस्य कार्यं न विद्यते ॥                  
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- Bhagavad Gita 3.17 ॥

(Read about this Shloka from the Bhagavad Gita here at [http://sanskritslokas.com/](http://sanskritslokas.com/gita-slokas2.html))